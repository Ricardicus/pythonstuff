What we've done so far:
- 1D root finding
- Multi-dim solutions (systems of eq.)
               ^  ^
       	Linear 			and	 		non-linear
       	Gaussian elmimination		FPI
       	LU decomposistion			Newton
       	Iterative

Issues with newton:
have to start near the root, jacobian complex to calculate and might not have an inverse

Issues with fpi:
The norm of the function G(X) must be less than one. Its a hard condition to be satisfied.

solve using gaussian elimination:
x1 + x2 = 3
3x1 - 4x2 = 2 
----------->
x1 = 2
x2 = 1

solve linear systems:

Idea Ax = B
1) Factor A = LU
Ax = B
LUx = b -> Ly = b, Ux = y
2) Solve Ly = b
3) Solve Ux = y 
Done, found x.

How to find LU? 

Doolittle's mothod to factor A = LU
L = (1 0 00 000 0. ...)
	(# 1 0 0 00 0 ....)
	(# # 1 0 0 0 0....)
.........................
L is like an identity matrix but with numbers below the diagonal. 
U is like an identity matrix but with numbers above the diagonal. 

1) Factor A = LU
A = [ 1  1 ] r1(-3)+r2 -> r2 gives U = 	[ 1  1 ]
	[ 3 -4 ]				 			[ 0 -7 ]

The factor used (-3 in this case) will be sign shifted and inserted in the vector L in the same place 
L = [ 1  0 ] 
	[ 3  1 ]

Now the matrixes U and L are found.
Ax = b
LUx = b
(Ux = y gives:)
Ly = b 
Ux = y

-> x = U^-1 * y = U^-1 * L^-1 * b

The trick here is that L and U contains a lot of zeros and are therefore much simpler to solve than the matrix A. 

Gaussian costs O(n^3) approx.
LU costs O(n^3) to factor LU = A but to solve Lx = y , Uy = b the cost is O(n^2)

The benefit is, if you are going to do this calculation with matrix A many times, 
you only have to calculate A = LU only once. Then the cost is O(n^2) witch is much better. 

Comparisons:
Gaussian elimination O(n^3)
LU decomposition O(n^3)
Iterative 		O(n^2)

Iterative
- Jacobi
- Richardson
- Gauss elimination

Ax = b
0 = -Ax + b
Q^-1 * Q = Q ^-1 (-Ax + Qx + b)
x = Q^-1Ax + Ox + Q^-1*b
x = (I - Q^-1*A)x + Q^-1*b
x_n+1 = G(x_n) + C

where the spectra radius of G is less than one (next time)
