Linear systems, Ax = b

Iterative schemes: (only approximate solutions, works on linear systems) 
- Jacobi
- Richardson
- Gauss-Seidel

x_n+1 = G(x_n) + C

( exakt solutions, the cost is in complexity O(n^3) )
- Gaussian Elimination 
- LU Facorization

The cost is measured in the number of multiplications (*) and divisions (/), this is what costs the most in time for the computer. 

Ax = b
0 = -Ax + b
Qx = Qx - Ax + b
Qx = (Q-A)x + b
x = Q^-1*(Q-A)x + Q^-1*b

where (I - Q^-1*A) = G
and Q^-1*b = C

x = Gx + C

If Q = D then => Jacobi methodq 
If Q = I then => Richardson
if Q = L+D    => Gauss-Seidel

What is L? 
It has nothing to do with LU-factorisation actually! L = lower, U = upper, D = diagonal
Ax = b	A			L 				D 			U
A = 	(1 2 3) => 	(0 0 0) 	+ 	(1 0 0)	+	(0 2 3)
    	(4 5 6)		(4 0 0)			(0 5 9)		(0 0 6)
    	(7 8 9)		(7 8 0)			(0 0 9)		(0 0 0)

Thm: If A is diagonally dominant then the sequence generated by x_n+1 = Gx_n + C conveges to the solution Ax = b for any starting vector x. 

Diagonally dominant?

A =	(4 0 1),	DD the value on the diagonal is the larges in every row. 
	(3 7 1)
	(6 0 7)

Thm: The iterative scheme x_n+1 = Gx_n + C converge to the unique solution of Ax = b for any initial x if and only if the spectral radius of G is less than 1. 

Spectral radius is the absolute value of the maximum eigen value of the matrix.

Example: Solve 	[1 2] [x1] = [1]
				[3 4] [x2]	 [2]

Jacobi method (don't confuse with jacobian matrix):
Q = D = [1 0]
		[0 4]

G = I - Q^-1*A = 	(1 0) - 1/4*(4 0)*(1 2) = 	(0   -2)
					(0 1)		(0 1) (3 4) = 	(-3/4 0)

C = 1/4 * 	(4 0)(1) = 1/4*(4) = 1/4 (4) = ( 1 )
			(0 1)(2)	   (2)		 (2)   (1/2)

Now both G and C is found. 

Jacobi method will not converge since max(eig(G)) = 1.2247 > 1
Gauss-Seidel is better.

G = I - Q^-1*A
  = (1 0 0) - 	(1 0 0)^-1 *(1 2 3) 
  	(0 1 0)		(0 5 0)		(4 5 6)
  	(0 0 1)		(0 0 9)		(7 8 9)

Jacobi on 	(1 2)(x1) = (1) :
			(3 4)(x2)	(2)

x1 = -2x2 + 1
x2 = -3/4x1 + 2

(x1) = 	(0   -2)(x1) + (1)
(x2)	(-3/4 0)(x2)   (2)

The jacobi solves for the diagonals of x.

costs:

The approximate solution methods all have O(n^2)*iterations in cost.

Example assume 3x3 matrix A. 
Use Jacobian to solve Ax=b, then the cost would be in O(n^2) approx. 9 operations * # iterations. If we assume 100 iterations are needed then Jacobi cost is 900 operations. 
If you use an exact method as Gaussian the cost is then n^3, approximately 27 operations. 

The dilemma is solved with the following theorem. 

Thm: Iterative schemes are better than Gaussian method if ln(tol)/ln(p(G)) < n/3 , n = size of A, p: spectral radius (max |lambda|). The tolerance is here the true tolerance, difference from the real root. But we can use it on the numerical tolerance aswell if we are to implement this computationally.  

Example:
Solve Ax = b where A is 30x30 and the spectral radius for Gauss-Seidel p(G) = 0.4.
Soppose we need tolerance 1e-5. Should you apply Gauss-Seidel or Gaussian elimination? 
  
>>> math.log(1e-5)/math.log(0.4)
12.564707973660301
 > 30 / 3 = 10 

So Gaussian elimination should be the choice. 

LU-factorisation is the best if Ax=b should be solved repeatedly. If it only should be solved once, then the LU-factorisation is just as good as the Gaussian elimination. 

Summary:

- GS is faster than Jacobi
- GS, Jacobi have a cost of O(n^2)
- Gaussian methods may be better depending on the size of the system
- Even faster methods exists (SSOR, SOR)

Ax = b
G = I  - Q^-1*A, we don't like calculating inverses. 
Condition number of a matrix determines what you should do. 
The condition number of the matrix is the norm of A times the norm of A inverse:
cond(a) = ||A||*||A^-1||, whatever norm. 
Example: If cond(A) = 10^k then be prepared to lose k digits of accuracy in computing the solution x. 

The condition number of the matrix can we not do anything about.
Swamping is a method to consider. 




